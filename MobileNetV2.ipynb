{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNArD7XwnrS8F6qW/wz1ew+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Train accuracy ~0.34 → 0.38 but flat\n",
        "\n",
        "Val accuracy stuck exactly at 0.4539\n",
        "\n",
        "Loss not improving\n",
        "\n",
        "This means → The model is not learning anything useful\n",
        "\n",
        "This usually happens when augmentation is too strong + backbone is frozen + small dataset."
      ],
      "metadata": {
        "id": "1nAwUi6AVdHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tmY3w79JQ1OQ",
        "outputId": "b792c83a-1ed9-4d65-d341-9762c0118368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found 761 files belonging to 3 classes.\n",
            "Using 609 files for training.\n",
            "Found 761 files belonging to 3 classes.\n",
            "Using 152 files for validation.\n",
            "Classes: ['Healthy', 'Mixed_Infected', 'Single_Infected']\n",
            "Train counts: {'Healthy': 240, 'Mixed_Infected': 246, 'Single_Infected': 123}\n",
            "Val counts: {'Healthy': 59, 'Mixed_Infected': 69, 'Single_Infected': 24}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ heavy_augmentation (\u001b[38;5;33mSequential\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multiply (\u001b[38;5;33mMultiply\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m327,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m771\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ heavy_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">771</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,586,691\u001b[0m (9.87 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,586,691</span> (9.87 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m328,707\u001b[0m (1.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">328,707</span> (1.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3449 - loss: 1.4346\n",
            "Epoch 1: val_accuracy improved from -inf to 0.45395, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - accuracy: 0.3452 - loss: 1.4336 - val_accuracy: 0.4539 - val_loss: 1.0200 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3384 - loss: 1.3125\n",
            "Epoch 2: val_accuracy did not improve from 0.45395\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 2s/step - accuracy: 0.3393 - loss: 1.3117 - val_accuracy: 0.4539 - val_loss: 1.0254 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3716 - loss: 1.2620\n",
            "Epoch 3: val_accuracy did not improve from 0.45395\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.3724 - loss: 1.2610 - val_accuracy: 0.4539 - val_loss: 1.0173 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3697 - loss: 1.2395\n",
            "Epoch 4: val_accuracy did not improve from 0.45395\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.3684 - loss: 1.2401 - val_accuracy: 0.4539 - val_loss: 1.0190 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3807 - loss: 1.2007\n",
            "Epoch 5: val_accuracy did not improve from 0.45395\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.3798 - loss: 1.2017 - val_accuracy: 0.4539 - val_loss: 1.0203 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3742 - loss: 1.1860\n",
            "Epoch 6: val_accuracy did not improve from 0.45395\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.3745 - loss: 1.1867 - val_accuracy: 0.4539 - val_loss: 1.0257 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3764 - loss: 1.1312\n",
            "Epoch 7: val_accuracy did not improve from 0.45395\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.3769 - loss: 1.1318 - val_accuracy: 0.4539 - val_loss: 1.0221 - learning_rate: 5.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3741 - loss: 1.1599\n",
            "Epoch 8: val_accuracy did not improve from 0.45395\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.3745 - loss: 1.1595 - val_accuracy: 0.4539 - val_loss: 1.0265 - learning_rate: 5.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m38/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3598 - loss: 1.1575"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3191888757.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# 11. Train head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m history_head = model.fit(\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    399\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                     )\n\u001b[0;32m--> 401\u001b[0;31m                 val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    402\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_evaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 1. Mount Drive (run cell)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set dataset directory\n",
        "DATA_DIR = '/content/drive/MyDrive/datasets'  # <- your folder\n",
        "\n",
        "# 2. Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# 3. Basic params\n",
        "IMG_SIZE = (224, 224)        # MobileNetV2 typical input\n",
        "BATCH_SIZE = 16\n",
        "SEED = 123\n",
        "EPOCHS_HEAD = 20             # train head\n",
        "EPOCHS_FINE = 10             # optional fine-tune\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# 4. Create tf.data datasets from directory (stratified by subfolders)\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# 5. Show class distribution (quick)\n",
        "def dataset_class_counts(dataset):\n",
        "    counts = {c:0 for c in class_names}\n",
        "    for images, labels in dataset.unbatch():\n",
        "        counts[class_names[int(labels.numpy())]] += 1\n",
        "    return counts\n",
        "\n",
        "print(\"Train counts:\", dataset_class_counts(train_ds))\n",
        "print(\"Val counts:\", dataset_class_counts(val_ds))\n",
        "\n",
        "# 6. Performance - prefetch\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# 7. Heavy augmentation (user choice). Put these layers into the model so augmentation is only applied during training.\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip('horizontal_and_vertical'),\n",
        "    layers.RandomRotation(0.25),   # up to 90 degrees total (0.25*2π? actually radians not used; it's fraction of 2π)\n",
        "    layers.RandomZoom(0.25),\n",
        "    layers.RandomTranslation(0.15, 0.15),\n",
        "    layers.RandomContrast(0.2),\n",
        "    # You can add Gaussian noise if desired:\n",
        "    layers.Rescaling(1./255),               # will later be countered by preprocess_input; ordering matters (we will preprocess_input after)\n",
        "], name='heavy_augmentation')\n",
        "\n",
        "# Note:\n",
        "# We include Rescaling here because some augmentation layers expect float inputs in [0,1].\n",
        "# But MobileNet's preprocess_input expects inputs scaled in a certain way. We'll still call preprocess_input later.\n",
        "\n",
        "# 8. Build model\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Backbone\n",
        "base_model = MobileNetV2(input_shape=(*IMG_SIZE, 3),\n",
        "                         include_top=False,\n",
        "                         weights='imagenet',\n",
        "                         pooling='avg')\n",
        "\n",
        "base_model.trainable = False  # freeze for initial training\n",
        "\n",
        "# Input layer\n",
        "inputs = keras.Input(shape=(*IMG_SIZE, 3))\n",
        "# Augmentation (applied only during training)\n",
        "x = layers.Rescaling(1./255)(inputs)   # ensure float [0,1] for augmentation\n",
        "x = data_augmentation(x)\n",
        "# MobileNetV2 expects images preprocessed with preprocess_input. Because we've rescaled to [0,1],\n",
        "# we need to bring them to the same scale MobileNet expects:\n",
        "x = preprocess_input(x * 255.0)  # revert to [0,255] then apply mobilenet preprocess\n",
        "x = base_model(x, training=False)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "# 9. Compile\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 10. Callbacks\n",
        "checkpoint_path = '/content/drive/MyDrive/mushroom_mobilenetv2_best.h5'\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "]\n",
        "\n",
        "# 11. Train head\n",
        "history_head = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_HEAD,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 12. Optional: fine-tune some top layers of the backbone\n",
        "# WARNING: Only do this if you have enough images (we will fine-tune carefully for a small dataset).\n",
        "# Unfreeze top layers of the base model:\n",
        "fine_tune_at = len(base_model.layers) - 30  # unfreeze last 30 layers\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[fine_tune_at:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# compile with lower lr\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=5e-6),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_HEAD + EPOCHS_FINE,\n",
        "    initial_epoch=history_head.epoch[-1] + 1 if history_head.epoch else 0,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# After training, the best weights are saved at checkpoint_path (also restored by EarlyStopping restore_best_weights=True)\n",
        "print(\"Best model saved to:\", checkpoint_path)\n",
        "\n",
        "# 13. Evaluate on validation set\n",
        "val_loss, val_acc = model.evaluate(val_ds)\n",
        "print(f\"Validation loss: {val_loss:.4f}, val accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# 14. Predictions + confusion matrix\n",
        "# Build arrays of true labels and predicted labels\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for images, labels in val_ds.unbatch():\n",
        "    images = tf.expand_dims(images, 0)  # add batch dimension\n",
        "    # preprocess same as model input pipeline:\n",
        "    img = tf.cast(images, tf.float32)\n",
        "    img = img / 255.0\n",
        "    img = preprocess_input(img * 255.0)\n",
        "    preds = model.predict(img, verbose=0)\n",
        "    y_true.append(int(labels.numpy()))\n",
        "    y_pred.append(int(np.argmax(preds, axis=-1)[0]))\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# 15. Plot confusion matrix (nice visualization)\n",
        "def plot_confusion_matrix(cm, classes):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(cm, class_names)\n",
        "\n",
        "# 16. Visualize augmented images (optional) - see what heavy augmentation produces\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,8))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3,3,i+1)\n",
        "        # apply the augmentation pipeline directly to a single image\n",
        "        img = images[i]\n",
        "        img_aug = data_augmentation(tf.expand_dims(img/255.0, 0))\n",
        "        # convert to uint8 for display\n",
        "        display_img = tf.cast(tf.clip_by_value(img_aug[0]*255.0, 0, 255), tf.uint8).numpy()\n",
        "        plt.imshow(display_img)\n",
        "        plt.axis('off')\n",
        "plt.suptitle('Examples of heavy augmentation')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Dataset path\n",
        "DATA_DIR = '/content/drive/MyDrive/datasets'  # <- adjust if needed\n",
        "\n",
        "# 2. Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "\n",
        "# 3. Parameters\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16\n",
        "SEED = 123\n",
        "EPOCHS = 20\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# 4. Load datasets\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# 5. Check class distribution\n",
        "def dataset_class_counts(dataset):\n",
        "    counts = {c:0 for c in class_names}\n",
        "    for _, labels in dataset.unbatch():\n",
        "        counts[class_names[int(labels.numpy())]] += 1\n",
        "    return counts\n",
        "\n",
        "print(\"Train counts:\", dataset_class_counts(train_ds))\n",
        "print(\"Val counts:\", dataset_class_counts(val_ds))\n",
        "\n",
        "# 6. Prefetch for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# 7. Lighter data augmentation\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomTranslation(0.1, 0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"light_augmentation\")\n",
        "\n",
        "# 8. Compute class weights\n",
        "labels_list = []\n",
        "for _, labels in train_ds.unbatch():\n",
        "    labels_list.append(int(labels.numpy()))\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(labels_list),\n",
        "    y=labels_list\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# 9. Build model\n",
        "num_classes = len(class_names)\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=(*IMG_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "# Unfreeze last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Input + augmentation\n",
        "inputs = keras.Input(shape=(*IMG_SIZE, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = data_augmentation(x)\n",
        "x = preprocess_input(x * 255.0)  # scale back to MobileNet input\n",
        "x = base_model(x, training=True)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "# 10. Compile\n",
        "optimizer = keras.optimizers.Adam(learning_rate=5e-5)  # fine-tuning learning rate\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 11. Callbacks\n",
        "checkpoint_path = '/content/drive/MyDrive/mushroom_mobilenetv2_best.h5'\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "]\n",
        "\n",
        "# 12. Train\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# 13. Evaluate\n",
        "val_loss, val_acc = model.evaluate(val_ds)\n",
        "print(f\"Validation loss: {val_loss:.4f}, val accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# 14. Predictions + confusion matrix\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in val_ds.unbatch():\n",
        "    img = tf.expand_dims(tf.cast(images, tf.float32)/255.0, 0)\n",
        "    img = preprocess_input(img * 255.0)\n",
        "    preds = model.predict(img, verbose=0)\n",
        "    y_true.append(int(labels.numpy()))\n",
        "    y_pred.append(int(np.argmax(preds, axis=-1)[0]))\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# 15. Confusion matrix plot\n",
        "def plot_confusion_matrix(cm, classes):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(cm, class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yxoFpGzDU_V0",
        "outputId": "d48c5417-e2aa-4c9b-a6c4-a0fe857c0252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 761 files belonging to 3 classes.\n",
            "Using 609 files for training.\n",
            "Found 761 files belonging to 3 classes.\n",
            "Using 152 files for validation.\n",
            "Classes: ['Healthy', 'Mixed_Infected', 'Single_Infected']\n",
            "Train counts: {'Healthy': 240, 'Mixed_Infected': 246, 'Single_Infected': 123}\n",
            "Val counts: {'Healthy': 59, 'Mixed_Infected': 69, 'Single_Infected': 24}\n",
            "Class weights: {0: np.float64(0.8458333333333333), 1: np.float64(0.8252032520325203), 2: np.float64(1.6504065040650406)}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_2 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ light_augmentation (\u001b[38;5;33mSequential\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ true_divide_1 (\u001b[38;5;33mTrueDivide\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subtract_1 (\u001b[38;5;33mSubtract\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m327,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m771\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ light_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ true_divide_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ subtract_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">771</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,586,691\u001b[0m (9.87 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,586,691</span> (9.87 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,552,579\u001b[0m (9.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,552,579</span> (9.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m34,112\u001b[0m (133.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,112</span> (133.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3790 - loss: 1.3233\n",
            "Epoch 1: val_accuracy improved from -inf to 0.55921, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 2s/step - accuracy: 0.3806 - loss: 1.3193 - val_accuracy: 0.5592 - val_loss: 0.9924 - learning_rate: 5.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5307 - loss: 0.9263\n",
            "Epoch 2: val_accuracy improved from 0.55921 to 0.57237, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.5315 - loss: 0.9253 - val_accuracy: 0.5724 - val_loss: 0.9766 - learning_rate: 5.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6061 - loss: 0.7102\n",
            "Epoch 3: val_accuracy improved from 0.57237 to 0.59211, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 2s/step - accuracy: 0.6068 - loss: 0.7103 - val_accuracy: 0.5921 - val_loss: 0.9411 - learning_rate: 5.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5816 - loss: 0.8397\n",
            "Epoch 4: val_accuracy improved from 0.59211 to 0.61842, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 2s/step - accuracy: 0.5834 - loss: 0.8360 - val_accuracy: 0.6184 - val_loss: 0.9295 - learning_rate: 5.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6865 - loss: 0.5994\n",
            "Epoch 5: val_accuracy improved from 0.61842 to 0.65132, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.6871 - loss: 0.5989 - val_accuracy: 0.6513 - val_loss: 0.8548 - learning_rate: 5.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7009 - loss: 0.5916\n",
            "Epoch 6: val_accuracy did not improve from 0.65132\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step - accuracy: 0.7017 - loss: 0.5905 - val_accuracy: 0.6513 - val_loss: 0.7920 - learning_rate: 5.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7207 - loss: 0.5240\n",
            "Epoch 7: val_accuracy did not improve from 0.65132\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 2s/step - accuracy: 0.7211 - loss: 0.5239 - val_accuracy: 0.6447 - val_loss: 0.8206 - learning_rate: 5.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7288 - loss: 0.5242\n",
            "Epoch 8: val_accuracy did not improve from 0.65132\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 2s/step - accuracy: 0.7294 - loss: 0.5230 - val_accuracy: 0.6118 - val_loss: 0.8823 - learning_rate: 5.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7482 - loss: 0.4967\n",
            "Epoch 9: val_accuracy did not improve from 0.65132\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step - accuracy: 0.7485 - loss: 0.4965 - val_accuracy: 0.6184 - val_loss: 0.9004 - learning_rate: 5.0000e-05\n",
            "Epoch 10/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7577 - loss: 0.4169\n",
            "Epoch 10: val_accuracy did not improve from 0.65132\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.7580 - loss: 0.4167 - val_accuracy: 0.6250 - val_loss: 0.8597 - learning_rate: 2.5000e-05\n",
            "Epoch 11/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8067 - loss: 0.4015\n",
            "Epoch 11: val_accuracy improved from 0.65132 to 0.65789, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.8064 - loss: 0.4015 - val_accuracy: 0.6579 - val_loss: 0.7918 - learning_rate: 2.5000e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8188 - loss: 0.3852\n",
            "Epoch 12: val_accuracy improved from 0.65789 to 0.67105, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.8190 - loss: 0.3851 - val_accuracy: 0.6711 - val_loss: 0.7485 - learning_rate: 2.5000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7652 - loss: 0.4212\n",
            "Epoch 13: val_accuracy did not improve from 0.67105\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step - accuracy: 0.7657 - loss: 0.4208 - val_accuracy: 0.6645 - val_loss: 0.7951 - learning_rate: 2.5000e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8194 - loss: 0.3732\n",
            "Epoch 14: val_accuracy did not improve from 0.67105\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.8195 - loss: 0.3730 - val_accuracy: 0.6447 - val_loss: 0.8263 - learning_rate: 2.5000e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7767 - loss: 0.4341\n",
            "Epoch 15: val_accuracy improved from 0.67105 to 0.68421, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2s/step - accuracy: 0.7771 - loss: 0.4338 - val_accuracy: 0.6842 - val_loss: 0.7681 - learning_rate: 2.5000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8575 - loss: 0.3068\n",
            "Epoch 16: val_accuracy improved from 0.68421 to 0.69079, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 2s/step - accuracy: 0.8571 - loss: 0.3078 - val_accuracy: 0.6908 - val_loss: 0.7482 - learning_rate: 1.2500e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8222 - loss: 0.3616\n",
            "Epoch 17: val_accuracy improved from 0.69079 to 0.70395, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 2s/step - accuracy: 0.8221 - loss: 0.3618 - val_accuracy: 0.7039 - val_loss: 0.7342 - learning_rate: 1.2500e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8111 - loss: 0.3741\n",
            "Epoch 18: val_accuracy improved from 0.70395 to 0.73026, saving model to /content/drive/MyDrive/mushroom_mobilenetv2_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.8112 - loss: 0.3737 - val_accuracy: 0.7303 - val_loss: 0.6885 - learning_rate: 1.2500e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8220 - loss: 0.3595\n",
            "Epoch 19: val_accuracy did not improve from 0.73026\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - accuracy: 0.8220 - loss: 0.3597 - val_accuracy: 0.7039 - val_loss: 0.7063 - learning_rate: 1.2500e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8147 - loss: 0.3500\n",
            "Epoch 20: val_accuracy did not improve from 0.73026\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 2s/step - accuracy: 0.8149 - loss: 0.3501 - val_accuracy: 0.7303 - val_loss: 0.6848 - learning_rate: 1.2500e-05\n",
            "Restoring model weights from the end of the best epoch: 20.\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.7489 - loss: 0.6907\n",
            "Validation loss: 0.6848, val accuracy: 0.7303\n",
            "Confusion matrix:\n",
            " [[ 0 59  0]\n",
            " [ 0 69  0]\n",
            " [ 0 24  0]]\n",
            "\n",
            "Classification report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        Healthy       0.00      0.00      0.00        59\n",
            " Mixed_Infected       0.45      1.00      0.62        69\n",
            "Single_Infected       0.00      0.00      0.00        24\n",
            "\n",
            "       accuracy                           0.45       152\n",
            "      macro avg       0.15      0.33      0.21       152\n",
            "   weighted avg       0.21      0.45      0.28       152\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileNetV2 backbone\n",
        "\n",
        "Lighter data augmentation\n",
        "\n",
        "Last 20 layers of MobileNetV2 unfrozen\n",
        "\n",
        "Class weights to handle imbalance\n",
        "\n",
        "Train + validation split from your Drive dataset"
      ],
      "metadata": {
        "id": "bWSDft0fVNIv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWfxaNqZVSQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}